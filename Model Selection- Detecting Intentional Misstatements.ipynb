{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detecting Intentional Misstatements by US Corporations\n",
    "\n",
    "**Motivation and objective:** Predicting whether a US corporation is intentionally misstating its financial statement is a supervised binary classification problem with imbalanced class distribution. This notebook aims to find the best machine learning model and hyperparameters to achieve a higher AUC score. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection\n",
    "\n",
    "The content of this notebook is as follow:\n",
    "\n",
    "- Dealing with missing data for test and train data set \n",
    "- Model selection and implementation\n",
    "- Model hyperparameter tuning with Randomized Grid CV search "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the relevant libraries\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "%matplotlib inline\n",
    "\n",
    "pd.set_option('display.max_colwidth',-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import testing and train data that contains all variables engineered \n",
    "training = pd.read_csv('training-merged.csv')\n",
    "testing = pd.read_csv('testing-merged.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imbalance class distribution: 0.02386117136659436\n"
     ]
    }
   ],
   "source": [
    "per_of_restatement  =len(training[training.Restate_Int == 1])/len(training)\n",
    "print('Imbalance class distribution:', per_of_restatement)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dealing with missing values \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove all the rows with missing values in any features before variable selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "training.dropna(axis = 0, inplace= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable Selection \n",
    "\n",
    "Based on the features identified from our exploratory data analytics, here are the variables that we will be including in our model: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variable selection here\n",
    "#creat X_train, y_train, X_test \n",
    "X_train = training[['Coleman_Liau_Index', 'averageWordsPerSentence', 'FinTerms_Negative',\n",
    "       'FinTerms_Positive', 'FinTerms_Litigious', 'FinTerms_ModalStrong',\n",
    "       'log_total_assets', 'change_in_inventory', 'percent_soft_assets',\n",
    "       'merger', 'big_n_auditor', 'tot_financing', 'ex_ante_financing',\n",
    "       'restructuring', 'auop_qualified', 'auopic_no_report',\n",
    "       'auopic_effective', 'auopic_adverse', 'au_9', 'ch_au', 'sic_2092',\n",
    "       'sic_2253', 'sic_3524', 'sic_3569', 'sic_4013', 'sic_4512', 'sic_5122',\n",
    "       'sic_5399', 'sic_6552', 'sic_8093','GOING_CONCERN']].copy()\n",
    "\n",
    "y_train= training['Restate_Int'].copy()\n",
    "\n",
    "X_test = testing[['Coleman_Liau_Index', 'averageWordsPerSentence', 'FinTerms_Negative',\n",
    "       'FinTerms_Positive', 'FinTerms_Litigious', 'FinTerms_ModalStrong',\n",
    "       'log_total_assets', 'change_in_inventory', 'percent_soft_assets',\n",
    "       'merger', 'big_n_auditor', 'tot_financing', 'ex_ante_financing',\n",
    "       'restructuring', 'auop_qualified', 'auopic_no_report',\n",
    "       'auopic_effective', 'auopic_adverse', 'au_9', 'ch_au', 'sic_2092',\n",
    "       'sic_2253', 'sic_3524', 'sic_3569', 'sic_4013', 'sic_4512', 'sic_5122',\n",
    "       'sic_5399', 'sic_6552', 'sic_8093','GOING_CONCERN']]\n",
    "\n",
    "output_gvkey = testing['Restate_Int']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replace all testing feature missing values: \n",
    "- if the feature is categorical, replace with most frequent value\n",
    "- if the feature is numeric, replace with mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill in all the missing values \n",
    "from sklearn.base import TransformerMixin\n",
    "class DataFrameImputer(TransformerMixin):\n",
    "    def __init__(self):\n",
    "        \"\"\"Impute missing values.\n",
    "        Columns of dtype object are imputed with the most frequent value \n",
    "        in column.\n",
    "        Columns of other types are imputed with mean of column.\n",
    "        \"\"\"\n",
    "    def fit(self, X, y=None):\n",
    "        self.fill = pd.Series([X[c].value_counts().index[0]\n",
    "            if X[c].dtype == np.dtype('O') else X[c].mean() for c in X],\n",
    "            index=X.columns)\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        return X.fillna(self.fill)\n",
    "X = pd.DataFrame(X_test)\n",
    "X_test = DataFrameImputer().fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14287\n",
      "14287\n",
      "2989\n",
      "2989\n"
     ]
    }
   ],
   "source": [
    "#make sure the length of the row matches\n",
    "print(len(X_train))\n",
    "print(len(y_train))\n",
    "print(len(X_test))\n",
    "print(len(output_gvkey))\n",
    "\n",
    "X_train.to_csv('X_train.csv')\n",
    "X_test.to_csv('X_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of Model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm 1:  Random forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Rational for selecting random forest: ** \n",
    "- Can be used for both continuos or categorical target prediction problem\n",
    "- Adopts bagging technique to combine multiples decision trees formed from subsets of the training set to improve the overall performance of the model \n",
    "- Bagging technique ensures minimum correlation among the trees which prevents overfitting, as tree methods used on its own tends to overfit\n",
    "- Good at detecting interactions between different features, but highly correlated features can mask these interactions\n",
    "- Able to handle a high number of features \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction with Random Forest's Default Setting\n",
    "With the default hyperparameters in Random Forest, the in sample accuracy is about 0.58 with 5 fold CV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Default hyperparameters in Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bootstrap': True,\n",
       " 'class_weight': None,\n",
       " 'criterion': 'gini',\n",
       " 'max_depth': None,\n",
       " 'max_features': 'auto',\n",
       " 'max_leaf_nodes': None,\n",
       " 'min_impurity_decrease': 0.0,\n",
       " 'min_impurity_split': None,\n",
       " 'min_samples_leaf': 1,\n",
       " 'min_samples_split': 2,\n",
       " 'min_weight_fraction_leaf': 0.0,\n",
       " 'n_estimators': 10,\n",
       " 'n_jobs': 1,\n",
       " 'oob_score': False,\n",
       " 'random_state': None,\n",
       " 'verbose': 0,\n",
       " 'warm_start': False}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a random forest object \n",
    "clf=RandomForestClassifier()\n",
    "# check the default paramater of random forest \n",
    "clf.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average 5-Fold CV Score: 0.5802198327632977\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf=RandomForestClassifier( )\n",
    "\n",
    "cv_scores = cross_val_score(clf, X_train, y_train, scoring = 'roc_auc', cv =5)\n",
    "print(\"Average 5-Fold CV Score: {}\".format(np.mean(cv_scores)))\n",
    "#y_pred=clf.predict(X_test)\n",
    "\n",
    "clf.fit(X_train,y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Hyperparameter Tuning in Random Forest with Random Search Cross Validation\n",
    "**Main hyperparameters to find tune:**\n",
    "- n_estimators = number of trees in the foreset\n",
    "- max_features = max number of features considered for splitting a node\n",
    "- max_depth = max number of levels in each decision tree\n",
    "- min_samples_split = min number of data points placed in a node before the node is split\n",
    "- min_samples_leaf = min number of data points allowed in a leaf node\n",
    "- bootstrap = method for sampling data points (with or without replacement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': [200, 288, 377, 466, 555, 644, 733, 822, 911, 1000], 'max_features': ['auto', 'sqrt'], 'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, None], 'min_samples_split': [2, 5, 10], 'min_samples_leaf': [1, 2, 4], 'bootstrap': [True, False]}\n"
     ]
    }
   ],
   "source": [
    "# prepare the hyperparameters to tune \n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 1000, num = 10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "\n",
    "print(random_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 100 candidates, totalling 200 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:  5.6min\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed: 22.8min\n",
      "[Parallel(n_jobs=-1)]: Done 200 out of 200 | elapsed: 28.0min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=2, error_score='raise',\n",
       "          estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False),\n",
       "          fit_params=None, iid=True, n_iter=100, n_jobs=-1,\n",
       "          param_distributions={'n_estimators': [200, 288, 377, 466, 555, 644, 733, 822, 911, 1000], 'max_features': ['auto', 'sqrt'], 'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, None], 'min_samples_split': [2, 5, 10], 'min_samples_leaf': [1, 2, 4], 'bootstrap': [True, False]},\n",
       "          pre_dispatch='2*n_jobs', random_state=42, refit=True,\n",
       "          return_train_score='warn', scoring='roc_auc', verbose=2)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_random = RandomizedSearchCV(estimator = clf,\n",
    "                               param_distributions = random_grid,\n",
    "                               n_iter = 100,\n",
    "                               cv = 2,\n",
    "                               verbose=2,\n",
    "                               random_state=42,\n",
    "                               n_jobs = -1,\n",
    "                               scoring = 'roc_auc')\n",
    "# Fit the random search model\n",
    "rf_random.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 911,\n",
       " 'min_samples_split': 5,\n",
       " 'min_samples_leaf': 4,\n",
       " 'max_features': 'auto',\n",
       " 'max_depth': 10,\n",
       " 'bootstrap': False}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the best parameter \n",
    "rf_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict \n",
    "pred = clf.predict_proba(X_test) \n",
    "#output the result to CSV \n",
    "\n",
    "results_df = pd.DataFrame(data={'gvkey':testing['gvkey'], 'Restate_Int':pred[:,1].tolist()})\n",
    "results_df.to_csv('rf_py.csv',index =False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_imp = pd.Series(clf.feature_importances_,index =X_train.columns).sort_values(ascending=False)\n",
    "feature_imp\n",
    "\n",
    "\n",
    "sns.barplot(x=feature_imp, y=feature_imp.index)\n",
    "# Add labels to your graph\n",
    "plt.rcParams.update({'font.size': 17})\n",
    "plt.rcParams['figure.figsize'] = [23, 20]\n",
    "\n",
    "plt.xlabel('Feature Importance Score')\n",
    "plt.ylabel('Features')\n",
    "plt.title(\"Visualizing Important Features\")\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm 2:  Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction with SVC's Default Setting\n",
    "With the default hyperparameters in SCV, the in sample accuracy is about 0.48 with 5 fold CV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Rational for selecting random forest: ** \n",
    "- One of the most robust binary classifcaiont models that is able to take in a large number of variales with high sparsity\n",
    "- Robust at dealing with datasets with imbalanced class frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 1.0,\n",
       " 'cache_size': 200,\n",
       " 'class_weight': None,\n",
       " 'coef0': 0.0,\n",
       " 'decision_function_shape': 'ovr',\n",
       " 'degree': 3,\n",
       " 'gamma': 'auto',\n",
       " 'kernel': 'rbf',\n",
       " 'max_iter': -1,\n",
       " 'probability': False,\n",
       " 'random_state': None,\n",
       " 'shrinking': True,\n",
       " 'tol': 0.001,\n",
       " 'verbose': False}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a random forest object \n",
    "clf = SVC()\n",
    "# check the default paramater of random forest \n",
    "clf.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average 5-Fold CV Score: 0.4800514951779018\n"
     ]
    }
   ],
   "source": [
    "clf = SVC(kernel = 'rbf') \n",
    "cv_results = cross_val_score(clf , X_train, y_train, cv =5, scoring = 'roc_auc')\n",
    "print(\"Average 5-Fold CV Score: {}\".format(np.mean(cv_results)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Hyperparameter Tuning in SVC with Random Search Cross Validation\n",
    "\n",
    "\n",
    "**The hyperparameter to fine tune in SVC are:** \n",
    "- Type of kernel \n",
    "- gamme = for non linear hyperplane\n",
    "- C = penality parameter of the error term, tradeoff bet smooth decision boundary and classifying the traning points correctly\n",
    "- Degree= degree of the polynomial used to find the hyperplane to split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gamma': [0.1, 1, 10, 100], 'C': [0.1, 1, 10, 100], 'degree': [0, 1, 2, 3, 4, 5, 6]}\n"
     ]
    }
   ],
   "source": [
    "# create the param for grid search \n",
    "gamma = [0.1,1,10,100 ]\n",
    "C = [0.1,1,10,100]\n",
    "degree = [0,1,2,3,4,5,6] #when degree is 1 , the kernel is same as linear \n",
    "\n",
    "param_grid = dict(gamma=gamma, C=C, degree= degree) #,degree=degree)\n",
    "print(param_grid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 50 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:  7.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.599750 using {'gamma': 10, 'degree': 1, 'C': 100}\n",
      "Execution time: 484.8979494571686 ms\n"
     ]
    }
   ],
   "source": [
    "import time \n",
    "\n",
    "# initiate a SVC obj\n",
    "clf = SVC() \n",
    "# Grid search \n",
    "grid = RandomizedSearchCV(estimator=clf, \n",
    "                          param_distributions = param_grid,\n",
    "                          n_iter = 50,\n",
    "                          cv = 2,\n",
    "                          verbose=2,\n",
    "                          random_state=42,\n",
    "                          n_jobs = -1,\n",
    "                          scoring = 'roc_auc') #no job to run in parallel, -1 means using all processor\n",
    "\n",
    "start_time = time.time()\n",
    "grid_result = grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "print(\"Execution time: \" + str((time.time() - start_time)) + ' ms')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best parameter selected is gamma of 10, degree of 1 and C of 100. These settings allows SVC to be non linear at fitting the training data set, penalizes error term by 100 which may lead to overfitting and uses 1 degree of polynomial used to find the hyperplane to split the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM]Average 5-Fold CV Score: 0.5852981260419671\n"
     ]
    }
   ],
   "source": [
    "#kfold = KFold(n_splits=3, random_state=7)\n",
    "\n",
    "clf = SVC( gamma =10 ,  C= 100 ,probability =True, degree =1 , verbose = True) \n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "cv_results = cross_val_score(clf , X_train, y_train, cv =5, scoring = 'roc_auc')\n",
    "\n",
    "print(\"Average 5-Fold CV Score: {}\".format(np.mean(cv_results)))\n",
    "\n",
    "output_test = clf.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.01019706, 0.02365011, 0.02375276, ..., 0.0244432 , 0.02432189,\n",
       "       0.02442388])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_test[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output the file \n",
    "results_df = pd.DataFrame(data={'gvkey':testing['gvkey'], 'Restate_Int':output_test[:,1]})\n",
    "results_df.to_csv('SVM_funetune_prob.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References: \n",
    "- https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74\n",
    "- https://medium.com/all-things-ai/in-depth-parameter-tuning-for-svc-758215394769\n",
    "- https://www.datacamp.com/community/tutorials/random-forests-classifier-python\n",
    "- https://statistics.berkeley.edu/sites/default/files/tech-reports/666.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
